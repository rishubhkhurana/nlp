{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2SeqCNNAttn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIB6B8Ez6eoKzZmj9bXKms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishubhkhurana/nlp/blob/main/cnn/Seq2SeqCNNAttn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADUMBPPbbS-Y"
      },
      "source": [
        "## Book Keeping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2BEuGrPbWtM"
      },
      "source": [
        "!python -m spacy download en > /dev/null 2>&1\n",
        "!python -m spacy download de > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0TKUzbDbuRO"
      },
      "source": [
        "## Importing Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_FBsh5Sbvjs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from torchtext.data import Field\n",
        "from torchtext.datasets import Multi30k\n",
        "import torchtext\n",
        "import random, time, math\n",
        "from torchtext.data import BucketIterator"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqXv33jCgHRY"
      },
      "source": [
        "## SEED everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJGy08AwgJG7"
      },
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tug0ZGLb9dh"
      },
      "source": [
        "## Dataset/ Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTnJr9vMcZsv"
      },
      "source": [
        "spacy_en = spacy.load('en')\n",
        "spacy_de = spacy.load('de')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXuDbCA1ce1s"
      },
      "source": [
        "def tokenize_src(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "def tokenize_trg(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIX0742Zb_Yw"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_src, init_token = '<sos>', \n",
        "            eos_token = '<eos>', lower = True,\n",
        "            batch_first = True)\n",
        "TRG = Field(tokenize = tokenize_trg, init_token = '<sos>',\n",
        "            eos_token = '<eos>', lower = True, batch_first = True)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql1fR_uGc_Wf"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(fields = (SRC, TRG), exts = ('.de', '.en'))"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_0U-GvNdZiP",
        "outputId": "d8b6c86c-8117-48a8-8961-9417e6ca55c1"
      },
      "source": [
        "\n",
        "len(train_data), len(valid_data), len(test_data)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29000, 1014, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNqNIIi_dqkx"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lgAvNjLcUuX"
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size = 128)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUW6ZciLeNPl"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaTgqDWxen78"
      },
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Class to encode the source language word sequence. \n",
        "    src words --> word embedding + position embedding --> linear layer --> CNN layers --> linear layer --> encoded\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 vocab_size,\n",
        "                 embed_dims,\n",
        "                 hidden_dims,\n",
        "                 ncnn_layers,\n",
        "                 max_length = 100,\n",
        "                 kernel_size = 3,\n",
        "                 dropout = 0.1,\n",
        "                 device = 'cuda'\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        # store all required variables\n",
        "        self.embed_dims = embed_dims\n",
        "        self.hidden_dims = hidden_dims\n",
        "        # define a scale variable which would be used when we add two latent variables\n",
        "        self.scale = torch.sqrt(torch.tensor([0.5])).float().to(device)\n",
        "        # add the token embedding \n",
        "        self.token_embedder = nn.Embedding(vocab_size, embed_dims)\n",
        "        # add position embedding, max length here refers to the source text's largest sequence length\n",
        "        self.position_embedder = nn.Embedding(max_length, embed_dims)\n",
        "        # add projection layer from embedding to hidden dims for conv layers\n",
        "        self.embed2hidden = nn.Linear(embed_dims, hidden_dims)\n",
        "        # add projection layer back from hidden to embedding after conv layers\n",
        "        self.hidden2embed = nn.Linear(hidden_dims, embed_dims)\n",
        "        # add the conv layers \n",
        "        # for conv layers, hidden dims are the number of input channels and we need to add 2*hidden dims \n",
        "        # we apply GLU activation by dividing the 2*hidden dims\n",
        "        # create an empy list to hold modules\n",
        "        self.convlayers = nn.ModuleList()\n",
        "        for _ in range(ncnn_layers):\n",
        "            self.convlayers.append(nn.Conv1d(in_channels = hidden_dims, \n",
        "                                             out_channels = 2*hidden_dims,\n",
        "                                             kernel_size = kernel_size, \n",
        "                                             stride = 1,\n",
        "                                             padding = kernel_size//2))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "\n",
        "    def forward(self, src):\n",
        "        # get the src seq length\n",
        "        # src: [Batch size, src length]\n",
        "        srclen = src.shape[1]\n",
        "        bs = src.shape[0]\n",
        "        # pass through the embedding layers\n",
        "        tok_embedded = self.dropout(self.token_embedder(src))\n",
        "        # tok_embedded : [batch size, src length, embed dims]\n",
        "        # pass through the position layers\n",
        "        positions = torch.arange(srclen).long().unsqueeze(0).repeat(bs, 1)\n",
        "        # positions: [bs, srclen]\n",
        "        pos_embedded = self.dropout(self.position_embedder(positions))\n",
        "        # pos_embedded: [bs, srclen, embed_dims]\n",
        "        # add the embeddings\n",
        "        combined_embedded = (pos_embedded+tok_embedded)\n",
        "        # combined_embedded: [bs, srclen, embed_dims]\n",
        "        # pass through the porjection layer prior to conv layers\n",
        "        conv_input = self.embed2hidden(combined_embedded)\n",
        "        # conv_input: [batch size, src length, hidden_dims]\n",
        "        # for conv layer, we need hidden dims as input channels and in pytorch, channels appear before the length/spatial dimen sions\n",
        "        conv_input = conv_input.permute(0, 2, 1)\n",
        "        # pass the conv input through the conv layers one by one\n",
        "        for clayer in self.convlayers:\n",
        "            conv_output = clayer(conv_input)\n",
        "            # conv_output: [batch size, 2*hidden dims, srclen]\n",
        "            conv_output = F.glu(conv_output, dim = 1)\n",
        "            # conv_output: [batch size, hidden_dims, srclen]\n",
        "            conv_output = self.dropout(conv_output)\n",
        "            # add residual connection \n",
        "            conv_output = (conv_output + conv_input)*self.scale\n",
        "            # store conv_output as conv input for next layer\n",
        "            conv_input = conv_output\n",
        "        # permute the dimensions\n",
        "        conv_output = conv_output.permute(0, 2, 1)\n",
        "        # conv_output: [batch size, srclen, hidden dims]\n",
        "        # pass the conv output through projection layer for conv output\n",
        "        conv_output = self.hidden2embed(conv_output)\n",
        "        # conv_output: [batch size, srclen, embed dims]\n",
        "        # add the original combined embedding as residual connection to conv output\n",
        "        combined = (combined_embedded + conv_output)*self.scale\n",
        "        \n",
        "        return combined, conv_output\n",
        "\n",
        "            \n"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUKolMIctP5f"
      },
      "source": [
        "class CNNDecoderWithAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embed_dims,\n",
        "                 hidden_dims,\n",
        "                 ncnn_layers,\n",
        "                 pad_idx,\n",
        "                 max_length=100,\n",
        "                 kernel_size = 3,\n",
        "                 dropout = 0.1,\n",
        "                 device = 'cuda'\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        # store the variables\n",
        "        self.kernel_size = kernel_size\n",
        "        self.scale = torch.sqrt(torch.tensor([0.5])).float().to(device)\n",
        "        self.embed_dims = embed_dims\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        # add the embedding layer for token\n",
        "        self.tok_embedder = nn.Embedding(vocab_size, embed_dims)\n",
        "        # add the embedding layer for positions\n",
        "        self.pos_embedder = nn.Embedding(max_length, embed_dims)\n",
        "        # add a projection layer before cnn layers\n",
        "        self.embed2hidden = nn.Linear(embed_dims, hidden_dims)\n",
        "        # add a projection layer for cnn output \n",
        "        self.hidden2embed = nn.Linear(hidden_dims, embed_dims)\n",
        "        # add an output layer \n",
        "        self.outlayer = nn.Linear(embed_dims, vocab_size)\n",
        "        # create an empty module list for cnn layers\n",
        "        self.convlayers = nn.ModuleList()\n",
        "        # add specified number of cnnlayers\n",
        "        for _ in range(ncnn_layers):\n",
        "            self.convlayers.append(nn.Conv1d(in_channels = hidden_dims,\n",
        "                                        out_channels = 2*hidden_dims, \n",
        "                                        kernel_size = kernel_size,\n",
        "                                        padding = 0))\n",
        "        # add dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # add attention projection layers\n",
        "        self.attnhid2embed = nn.Linear(hidden_dims, embed_dims)\n",
        "        self.attnembed2hid = nn.Linear(embed_dims, hidden_dims)\n",
        "\n",
        "\n",
        "    def get_attention(self, combined_embedding, conv_output, encoder_conv_output, encoder_combined):\n",
        "        # encoder_conv_output: [batch size, srclen, embed_dims]\n",
        "        # encoder_combined: [batch size, srclen, embed_dims]\n",
        "        # combined_embedding: [batch size, trglen, embed_dims]\n",
        "        # conv_output: [batch size, hidden_dims, trglen]\n",
        "        # assumption --> embed_dims for encoder are same as embed_dims for decoder\n",
        "        # project the conv_output to embed dims\n",
        "        conv_attention = self.attnhid2embed(conv_output.permute(0, 2, 1))\n",
        "\n",
        "        # add the target embedding to conv_attention\n",
        "        combined_attention = (combined_embedding + conv_attention)*self.scale\n",
        "\n",
        "        # compute the attention raw scores with encoder \n",
        "        attention_energy = torch.matmul(combined_attention, encoder_conv_output.permute(0, 2, 1))\n",
        "        # attention_energy: [bs, trglen, srclen]\n",
        "\n",
        "        # apply softmax to get the attention probabilities\n",
        "        attention = F.softmax(attention_energy, dim = 2)\n",
        "\n",
        "        # multy the attention probabilties with encoder combined to get then attention value vector \n",
        "        attention_values = torch.matmul(attention, encoder_combined)\n",
        "        # project the attention values back to hidden dims\n",
        "        # attention_values: [bs, trglen, embed_dims]\n",
        "        attention_values = self.attnembed2hid(attention_values).permute(0, 2, 1)\n",
        "        # attention_values: [bs, hidden_dims, trglen]\n",
        "\n",
        "        attention_combined = (conv_output + attention_values)*self.scale\n",
        "        # attention_combined: [bs, trglen, hidden_dims]\n",
        "\n",
        "        return attention, attention_combined\n",
        "    \n",
        "    def forward(self, trg, encoder_combined, encoder_conv_output):\n",
        "\n",
        "        # trg: [bs, trglen]\n",
        "        trglen = trg.shape[1]\n",
        "        bs = trg.shape[0]\n",
        "\n",
        "        # get the token embedding\n",
        "        tok_embedding = self.dropout(self.tok_embedder(trg))\n",
        "        # get position embedding\n",
        "        positions = torch.arange(trglen).repeat(bs, 1)\n",
        "        pos_embedding = self.dropout(self.pos_embedder(positions))\n",
        "        # combined embedding\n",
        "        combined_embedding = tok_embedding + pos_embedding \n",
        "        # combined_embedding: [bs, trglen, embed_dims]\n",
        "\n",
        "        # project the embedding before passing through conv layers\n",
        "        conv_input = self.embed2hidden(combined_embedding)\n",
        "        # conv_input: [bs, trglen, hidden_dims]\n",
        "        conv_input = conv_input.permute(0, 2, 1)\n",
        "        # conv_input: [bs, hidden_dims, trglen]\n",
        "\n",
        "        # pass through conv layers\n",
        "        for clayer in self.convlayers:\n",
        "            padding = torch.zeros(bs, self.hidden_dims, self.kernel_size-1).fill_(self.pad_idx).to(self.device)\n",
        "            padded_conv_input = torch.cat([padding, conv_input], dim = 2)\n",
        "            # padded_conv_input: [bs, hidden_dims, trglen+self.kernel_size-1]\n",
        "            conv_output = clayer(padded_conv_input)\n",
        "            # conv_output: [bs, 2*hidden_dims, trglen]\n",
        "            conv_output = F.glu(conv_output, dim = 1)\n",
        "            # conv_output: [bs, hidden_dims, trglen]\n",
        "            attention, conv_output = self.get_attention(combined_embedding, conv_output, encoder_combined, encoder_conv_output)\n",
        "            # attention: [bs, trglen, srclen]\n",
        "            # conv_output: [bs, hidden_dims, trglen]\n",
        "            conv_output = (conv_input + conv_output)*self.scale\n",
        "            conv_input = conv_output\n",
        "        \n",
        "        # conv_output: [bs, hidden_dims, trglen]\n",
        "        conv_output = conv_output.permute(0, 2, 1)\n",
        "        # conv_output: [bs, trglen, hidden_dims]\n",
        "        # pass through the re-projection layer\n",
        "        conv_output = self.hidden2embed(conv_output)\n",
        "        # conv_output: [bs, trglen, embed_dims]\n",
        "\n",
        "        # compute the output raw scores \n",
        "        classifier_output = self.outlayer(conv_output)\n",
        "        # classifier_output: [bs, trglen, vocab_size]\n",
        "\n",
        "        return classifier_output, attention"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23_d5Z1kUxV_"
      },
      "source": [
        "class CNNSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        # register encoder\n",
        "        self.encoder = encoder\n",
        "        # register decoder\n",
        "        self.decoder = decoder\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        # src: [bs, srclen]\n",
        "        # trg: [bs, trglen]\n",
        "        encoder_combined, encoder_conv_output = self.encoder(src)\n",
        "        # encoder_combined: [bs, srclen, embed_dims]\n",
        "        # encoder_conv_output: [bs, srclen, embed_dims]\n",
        "        output, attention = self.decoder(trg, encoder_combined, encoder_conv_output)\n",
        "        return output, attention"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10_kFjjnWW0j"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brUZYdwIa1dd"
      },
      "source": [
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyKF8gCLWYxl"
      },
      "source": [
        "\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        " \n",
        "enc = CNNEncoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, dropout = ENC_DROPOUT, device = device, max_length = 500)\n",
        "dec = CNNDecoderWithAttention(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, dropout = DEC_DROPOUT, pad_idx = TRG_PAD_IDX, device = device, max_length = 500)\n",
        "\n",
        "model = CNNSeq2Seq(enc, dec).to(device)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1hwtYlSauFH",
        "outputId": "f8a38478-ffc3-4366-a2c2-06faa85fba13"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "model"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNSeq2Seq(\n",
              "  (encoder): CNNEncoder(\n",
              "    (token_embedder): Embedding(7855, 256)\n",
              "    (position_embedder): Embedding(500, 256)\n",
              "    (embed2hidden): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (hidden2embed): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (convlayers): ModuleList(\n",
              "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (8): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (9): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    )\n",
              "    (dropout): Dropout(p=0.25, inplace=False)\n",
              "  )\n",
              "  (decoder): CNNDecoderWithAttention(\n",
              "    (tok_embedder): Embedding(5893, 256)\n",
              "    (pos_embedder): Embedding(500, 256)\n",
              "    (embed2hidden): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (hidden2embed): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (outlayer): Linear(in_features=256, out_features=5893, bias=True)\n",
              "    (convlayers): ModuleList(\n",
              "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (8): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "      (9): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
              "    )\n",
              "    (dropout): Dropout(p=0.25, inplace=False)\n",
              "    (attnhid2embed): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (attnembed2hid): Linear(in_features=256, out_features=512, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9JAhDLKb2mH",
        "outputId": "fdc5b577-34a3-459d-cda4-71d79bdd0408"
      },
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 37,556,485 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-69qEKSb6De"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We1MaJzYb8t-"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHkcibddcACI"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XynZ0wS9cEye"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 0.1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss} | Train PPL: {math.exp(train_loss)}')\n",
        "    print(f'\\t Val. Loss: {valid_loss} |  Val. PPL: {math.exp(valid_loss)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TM_AXB7cFQi"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}